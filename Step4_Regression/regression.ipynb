{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 4: Linear Regression with Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we intend to predict the score called \"like_x\", which means how much this person likes his/her date. After building these models, we wish to predict the \"like_x\" score based on the information about a potential dating pair. This prediction can be used to select pairs before holding the speed dating event. The models we will use are Linear Regression with Lasso and Linear Regression with Ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Read the data from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7278, 189)\n"
     ]
    }
   ],
   "source": [
    "#Starting df_new.shape\n",
    "df_new = pd.read_csv(\"merged_df.csv\")\n",
    "print df_new.shape\n",
    "dftrain = pd.read_csv(\"merged_dftrain.csv\")\n",
    "dftest  = pd.read_csv(\"merged_dftest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Do feature selection for trainng regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n",
      "184\n",
      "3\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Get all column names\n",
    "print len(list(df_new.columns.values))\n",
    "\n",
    "\n",
    "#Get potential regression features\n",
    "reg_features = ['gender_x',\n",
    "                'sum_in_z', 'age_d_z', 'imprace_d_z', 'imprelig_d_z', 'date_d_z', 'go_out_d_z', 'goal_d_z', 'career_d_z', 'field_d_z', 'race_d_z', 'from_d_z',\n",
    "                'age_x', 'imprace_x', 'imprelig_x', 'date_x', 'go_out_x', 'sports_x', 'tvsports_x', 'exercise_x', 'dining_x', 'museums_x', 'art_x', 'hiking_x', 'gaming_x', 'clubbing_x', 'reading_x', 'tv_x', 'theater_x', 'movies_x', 'concerts_x', 'music_x', 'shopping_x', 'yoga_x', 'attr1_1_x', 'sinc1_1_x', 'intel1_1_x', 'fun1_1_x', 'amb1_1_x', 'shar1_1_x', 'attr2_1_x', 'sinc2_1_x', 'intel2_1_x', 'fun2_1_x', 'amb2_1_x', 'shar2_1_x', 'attr3_1_x', 'sinc3_1_x', 'fun3_1_x', 'intel3_1_x', 'amb3_1_x', \n",
    "                'f1law_x', 'f2math_x', 'f3social_sci_x', 'f4medical_sci_x', 'f5engineering_x', 'f6english_x', 'f7hist_x', 'f8business_x', 'f9edu_x', 'f10bio_sci_x', 'f11social_work_x', 'f12undergrad_x', 'f13poli_sci_x', 'f14film_x', 'f15arts_x', 'f16languages_x', 'f17architecture_x', 'f18other_x', 'r1black_x', 'r2european_x', 'r3latino_x', 'r4asian_x', 'r6other_x', 'c1lawyer_x', 'c2academic_x', 'c3psycho_x', 'c4doctor_x', 'c5engineer_x', 'c6entertainment_x', 'c7banking_x', 'c8real_estate_x', 'c9inter_affairs_x', 'c10undeci_x', 'c11social_work_x', 'c12speech_x', 'c13politics_x', 'c14sports_x', 'c15other_x', 'c16journalism_x', 'c17architecture_x', 'goal_s_x', \n",
    "                'age_y', 'imprace_y', 'imprelig_y', 'date_y', 'go_out_y', 'sports_y', 'tvsports_y', 'exercise_y', 'dining_y', 'museums_y', 'art_y', 'hiking_y', 'gaming_y', 'clubbing_y', 'reading_y', 'tv_y', 'theater_y', 'movies_y', 'concerts_y', 'music_y', 'shopping_y', 'yoga_y', 'attr1_1_y', 'sinc1_1_y', 'intel1_1_y', 'fun1_1_y', 'amb1_1_y', 'shar1_1_y', 'attr2_1_y', 'sinc2_1_y', 'intel2_1_y', 'fun2_1_y', 'amb2_1_y', 'shar2_1_y', 'attr3_1_y', 'sinc3_1_y', 'fun3_1_y', 'intel3_1_y', 'amb3_1_y',\n",
    "                'f1law_y', 'f2math_y', 'f3social_sci_y', 'f4medical_sci_y', 'f5engineering_y', 'f6english_y', 'f7hist_y', 'f8business_y', 'f9edu_y', 'f10bio_sci_y', 'f11social_work_y', 'f12undergrad_y', 'f13poli_sci_y', 'f14film_y', 'f15arts_y', 'f16languages_y', 'f17architecture_y', 'f18other_y', 'r1black_y', 'r2european_y', 'r3latino_y', 'r4asian_y', 'r6other_y', 'c1lawyer_y', 'c2academic_y', 'c3psycho_y', 'c4doctor_y', 'c5engineer_y', 'c6entertainment_y', 'c7banking_y', 'c8real_estate_y', 'c9inter_affairs_y', 'c10undeci_y', 'c11social_work_y', 'c12speech_y', 'c13politics_y', 'c14sports_y', 'c15other_y', 'c16journalism_y', 'c17architecture_y', 'goal_s_y',\n",
    "\n",
    "               ]\n",
    "#plus the following features\n",
    "#'attr_x', 'sinc_x', 'intel_x', 'fun_x', 'amb_x', 'shar_x', \n",
    "#'attr_y', 'sinc_y', 'intel_y', 'fun_y', 'amb_y', 'shar_y',\n",
    "reg_features_pl = ['gender_x',\n",
    "                'sum_in_z', 'age_d_z', 'imprace_d_z', 'imprelig_d_z', 'date_d_z', 'go_out_d_z', 'goal_d_z', 'career_d_z', 'field_d_z', 'race_d_z', 'from_d_z',\n",
    "                'age_x', 'imprace_x', 'imprelig_x', 'date_x', 'go_out_x', 'sports_x', 'tvsports_x', 'exercise_x', 'dining_x', 'museums_x', 'art_x', 'hiking_x', 'gaming_x', 'clubbing_x', 'reading_x', 'tv_x', 'theater_x', 'movies_x', 'concerts_x', 'music_x', 'shopping_x', 'yoga_x', 'attr1_1_x', 'sinc1_1_x', 'intel1_1_x', 'fun1_1_x', 'amb1_1_x', 'shar1_1_x', 'attr2_1_x', 'sinc2_1_x', 'intel2_1_x', 'fun2_1_x', 'amb2_1_x', 'shar2_1_x', 'attr3_1_x', 'sinc3_1_x', 'fun3_1_x', 'intel3_1_x', 'amb3_1_x', 'attr_x', 'sinc_x', 'intel_x', 'fun_x', 'amb_x', 'shar_x',\n",
    "                'f1law_x', 'f2math_x', 'f3social_sci_x', 'f4medical_sci_x', 'f5engineering_x', 'f6english_x', 'f7hist_x', 'f8business_x', 'f9edu_x', 'f10bio_sci_x', 'f11social_work_x', 'f12undergrad_x', 'f13poli_sci_x', 'f14film_x', 'f15arts_x', 'f16languages_x', 'f17architecture_x', 'f18other_x', 'r1black_x', 'r2european_x', 'r3latino_x', 'r4asian_x', 'r6other_x', 'c1lawyer_x', 'c2academic_x', 'c3psycho_x', 'c4doctor_x', 'c5engineer_x', 'c6entertainment_x', 'c7banking_x', 'c8real_estate_x', 'c9inter_affairs_x', 'c10undeci_x', 'c11social_work_x', 'c12speech_x', 'c13politics_x', 'c14sports_x', 'c15other_x', 'c16journalism_x', 'c17architecture_x', 'goal_s_x', \n",
    "                'age_y', 'imprace_y', 'imprelig_y', 'date_y', 'go_out_y', 'sports_y', 'tvsports_y', 'exercise_y', 'dining_y', 'museums_y', 'art_y', 'hiking_y', 'gaming_y', 'clubbing_y', 'reading_y', 'tv_y', 'theater_y', 'movies_y', 'concerts_y', 'music_y', 'shopping_y', 'yoga_y', 'attr1_1_y', 'sinc1_1_y', 'intel1_1_y', 'fun1_1_y', 'amb1_1_y', 'shar1_1_y', 'attr2_1_y', 'sinc2_1_y', 'intel2_1_y', 'fun2_1_y', 'amb2_1_y', 'shar2_1_y', 'attr3_1_y', 'sinc3_1_y', 'fun3_1_y', 'intel3_1_y', 'amb3_1_y', 'attr_y', 'sinc_y', 'intel_y', 'fun_y', 'amb_y', 'shar_y',\n",
    "                'f1law_y', 'f2math_y', 'f3social_sci_y', 'f4medical_sci_y', 'f5engineering_y', 'f6english_y', 'f7hist_y', 'f8business_y', 'f9edu_y', 'f10bio_sci_y', 'f11social_work_y', 'f12undergrad_y', 'f13poli_sci_y', 'f14film_y', 'f15arts_y', 'f16languages_y', 'f17architecture_y', 'f18other_y', 'r1black_y', 'r2european_y', 'r3latino_y', 'r4asian_y', 'r6other_y', 'c1lawyer_y', 'c2academic_y', 'c3psycho_y', 'c4doctor_y', 'c5engineer_y', 'c6entertainment_y', 'c7banking_y', 'c8real_estate_y', 'c9inter_affairs_y', 'c10undeci_y', 'c11social_work_y', 'c12speech_y', 'c13politics_y', 'c14sports_y', 'c15other_y', 'c16journalism_y', 'c17architecture_y', 'goal_s_y',\n",
    "               ]\n",
    "reg_nonfeatures = ['iid', 'piid','match_x', \n",
    "                  ]\n",
    "print len(reg_features_pl)\n",
    "print len(reg_nonfeatures)\n",
    "\n",
    "\n",
    "#Get responses\n",
    "reg_response_like = ['like_x',]\n",
    "reg_response_prob = ['prob_x',]\n",
    "\n",
    "print len(reg_response_like)\n",
    "print len(reg_response_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Check if all features have been filtered\n",
    "print len(list(df_new.columns.values))==( len(reg_features_pl)+len(reg_nonfeatures)+len(reg_response_like)+len(reg_response_prob) )\n",
    "\n",
    "for item in list(df_new.columns.values):\n",
    "    if (item not in reg_features_pl) and (item not in reg_nonfeatures) and (item not in reg_response_like) and (item not in reg_response_prob):\n",
    "        print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_function = \"mean_squared_error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Regression with regularization\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "def cv_optimize_ridge(X, y, n_folds=4):\n",
    "    clf = Ridge()\n",
    "    parameters = {\"alpha\": [1e-10, 1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\n",
    "    #the scoring parameter below is the default one in ridge, but you can use a different one\n",
    "    #in the cross-validation phase if you want.\n",
    "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, scoring=score_function)\n",
    "    gs.fit(X, y)\n",
    "    return gs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Regression with lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "# from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "def cv_optimize_lasso(X, y, n_folds=4):\n",
    "    clf = Lasso()\n",
    "    parameters = {\"alpha\": [1e-10, 1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\n",
    "    #the scoring parameter below is the default one in lasso, but you can use a different one\n",
    "    #in the cross-validation phase if you want.\n",
    "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, scoring=score_function)\n",
    "    gs.fit(X, y)\n",
    "    return gs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict 'like_x'\n",
    "\n",
    "### 'like_x' refers to \"overall, how much do you like this person\", which we would like to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing regression on the dataset to make prediction about 'like_x' and 'prob_x', we found 'Attractive'(attr), 'Sincere'(sinc), 'Intelligent'(intel), 'Fun'(fun), 'Ambitious'(amb), 'Shared Interests/Hobbies'(shar) very helpful in training a good regression model. \n",
    "\n",
    "These features are how participants of speed dating rate their partner's attributes on a scale of 1-10: (1=awful, 10=great)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Does not consider the following features:\n",
    "\n",
    "####'attr_x', 'sinc_x', 'intel_x', 'fun_x', 'amb_x', 'shar_x', \n",
    "\n",
    "####'attr_y', 'sinc_y', 'intel_y', 'fun_y', 'amb_y', 'shar_y',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5094, 172)\n",
      "(2184, 172)\n",
      "(5094, 1)\n",
      "(2184, 1)\n"
     ]
    }
   ],
   "source": [
    "#The first pair(ridge + lasso) of model: \n",
    "#Predict 'like_x'\n",
    "#Does not consider the following features:\n",
    "#'attr_x', 'sinc_x', 'intel_x', 'fun_x', 'amb_x', 'shar_x', \n",
    "#'attr_y', 'sinc_y', 'intel_y', 'fun_y', 'amb_y', 'shar_y',\n",
    "\n",
    "X_train = dftrain[reg_features]\n",
    "X_test  = dftest[reg_features]\n",
    "Y_train = dftrain[reg_response_like]\n",
    "Y_test  = dftest[reg_response_like]\n",
    "\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print Y_train.shape\n",
    "print Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1.  Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, solver='auto', tol=0.001) {'alpha': 1.0} -1.03167342452 [mean: -1.03691, std: 0.09697, params: {'alpha': 1e-10}, mean: -1.03691, std: 0.09697, params: {'alpha': 1e-08}, mean: -1.03691, std: 0.09697, params: {'alpha': 1e-06}, mean: -1.03691, std: 0.09697, params: {'alpha': 1e-05}, mean: -1.03691, std: 0.09697, params: {'alpha': 5e-05}, mean: -1.03691, std: 0.09697, params: {'alpha': 0.0001}, mean: -1.03690, std: 0.09697, params: {'alpha': 0.0005}, mean: -1.03690, std: 0.09696, params: {'alpha': 0.001}, mean: -1.03685, std: 0.09695, params: {'alpha': 0.01}, mean: -1.03632, std: 0.09680, params: {'alpha': 0.1}, mean: -1.03167, std: 0.09560, params: {'alpha': 1.0}]\n",
      "train mean squared error: 0.829362496015\n",
      "test mean squared error: 0.922679146417 \n",
      "\n",
      "train score: 0.170637503985\n",
      "test score: 0.111518392809 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The ridge regression model\n",
    "fitmodel = cv_optimize_ridge(X_train, Y_train, n_folds=4)\n",
    "\n",
    "print fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_\n",
    "\n",
    "alphawechoose = fitmodel.best_params_['alpha']\n",
    "clf_reg = Ridge(alpha=alphawechoose).fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "errtr=mean_squared_error(clf_reg.predict(X_train), Y_train)\n",
    "err=mean_squared_error(clf_reg.predict(X_test), Y_test)\n",
    "\n",
    "print 'train mean squared error:', errtr\n",
    "print 'test mean squared error:', err, '\\n'\n",
    "\n",
    "training_accuracy = clf_reg.score(X_train, Y_train)\n",
    "test_accuracy = clf_reg.score(X_test, Y_test)\n",
    "\n",
    "print 'train score:', training_accuracy\n",
    "print 'test score:', test_accuracy, '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Ridge Regression, the train score is 0.17 and the test score is 0.11. Both are quite low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.  Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False) {'alpha': 0.01} -0.985866984021 [mean: -1.03681, std: 0.09751, params: {'alpha': 1e-10}, mean: -1.03681, std: 0.09751, params: {'alpha': 1e-08}, mean: -1.03675, std: 0.09748, params: {'alpha': 1e-06}, mean: -1.03624, std: 0.09724, params: {'alpha': 1e-05}, mean: -1.03457, std: 0.09652, params: {'alpha': 5e-05}, mean: -1.03226, std: 0.09638, params: {'alpha': 0.0001}, mean: -1.01669, std: 0.09652, params: {'alpha': 0.0005}, mean: -1.00334, std: 0.09848, params: {'alpha': 0.001}, mean: -0.98587, std: 0.09897, params: {'alpha': 0.01}, mean: -1.00849, std: 0.07382, params: {'alpha': 0.1}, mean: -1.01064, std: 0.07205, params: {'alpha': 1.0}]\n",
      "train mean squared error: 0.881597404859\n",
      "test mean squared error: 0.947058896231 \n",
      "\n",
      "train score: 0.118402595141\n",
      "test score: 0.0880422371143 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feifeipeng/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:444: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#The lasso regression model\n",
    "fitmodel = cv_optimize_lasso(X_train, Y_train, n_folds=4)\n",
    "\n",
    "print fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_\n",
    "\n",
    "alphawechoose = fitmodel.best_params_['alpha']\n",
    "clf_lasso = Lasso(alpha=alphawechoose).fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "errtr=mean_squared_error(clf_lasso.predict(X_train), Y_train)\n",
    "err=mean_squared_error(clf_lasso.predict(X_test), Y_test)\n",
    "\n",
    "print 'train mean squared error:', errtr\n",
    "print 'test mean squared error:', err, '\\n'\n",
    "\n",
    "training_accuracy = clf_lasso.score(X_train, Y_train)\n",
    "test_accuracy = clf_lasso.score(X_test, Y_test)\n",
    "\n",
    "print 'train score:', training_accuracy\n",
    "print 'test score:', test_accuracy, '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Lasso, we get training score 0.12 and testing score 0.09. Which are still very low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see what difference would the models bring if we add the 12 features, which are ratings each pariticipants gave to/received from his/her date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Consider the following features:\n",
    "\n",
    "####'attr_x', 'sinc_x', 'intel_x', 'fun_x', 'amb_x', 'shar_x', \n",
    "\n",
    "####'attr_y', 'sinc_y', 'intel_y', 'fun_y', 'amb_y', 'shar_y',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5094, 184)\n",
      "(2184, 184)\n",
      "(5094, 1)\n",
      "(2184, 1)\n"
     ]
    }
   ],
   "source": [
    "#The second pair(ridge + lasso) of model: \n",
    "#Predict 'like_x'\n",
    "#Consider the following features:\n",
    "#'attr_x', 'sinc_x', 'intel_x', 'fun_x', 'amb_x', 'shar_x', \n",
    "#'attr_y', 'sinc_y', 'intel_y', 'fun_y', 'amb_y', 'shar_y',\n",
    "\n",
    "X_train = dftrain[reg_features_pl]\n",
    "X_test  = dftest[reg_features_pl]\n",
    "Y_train = dftrain[reg_response_like]\n",
    "Y_test  = dftest[reg_response_like]\n",
    "\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print Y_train.shape\n",
    "print Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, solver='auto', tol=0.001) {'alpha': 1.0} -0.400704672548 [mean: -0.40186, std: 0.01921, params: {'alpha': 1e-10}, mean: -0.40186, std: 0.01922, params: {'alpha': 1e-08}, mean: -0.40186, std: 0.01922, params: {'alpha': 1e-06}, mean: -0.40186, std: 0.01922, params: {'alpha': 1e-05}, mean: -0.40186, std: 0.01922, params: {'alpha': 5e-05}, mean: -0.40186, std: 0.01922, params: {'alpha': 0.0001}, mean: -0.40186, std: 0.01922, params: {'alpha': 0.0005}, mean: -0.40186, std: 0.01922, params: {'alpha': 0.001}, mean: -0.40184, std: 0.01921, params: {'alpha': 0.01}, mean: -0.40172, std: 0.01917, params: {'alpha': 0.1}, mean: -0.40070, std: 0.01891, params: {'alpha': 1.0}]\n",
      "train mean squared error: 0.328627292591\n",
      "test mean squared error: 0.353045103544 \n",
      "\n",
      "train score: 0.671372707409\n",
      "test score: 0.66003991504 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The ridge regression model\n",
    "fitmodel = cv_optimize_ridge(X_train, Y_train, n_folds=4)\n",
    "\n",
    "print fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_\n",
    "\n",
    "alphawechoose = fitmodel.best_params_['alpha']\n",
    "clf_reg = Ridge(alpha=alphawechoose).fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "errtr=mean_squared_error(clf_reg.predict(X_train), Y_train)\n",
    "err=mean_squared_error(clf_reg.predict(X_test), Y_test)\n",
    "\n",
    "print 'train mean squared error:', errtr\n",
    "print 'test mean squared error:', err, '\\n'\n",
    "\n",
    "training_accuracy = clf_reg.score(X_train, Y_train)\n",
    "test_accuracy = clf_reg.score(X_test, Y_test)\n",
    "\n",
    "print 'train score:', training_accuracy\n",
    "print 'test score:', test_accuracy, '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Ridge Regression, the train score increases from 0.17 to **0.67**, and the test score increases from 0.11 to **0.66**. The improvement is huge adding these 12 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False) {'alpha': 0.01} -0.3723075204 [mean: -0.40204, std: 0.01918, params: {'alpha': 1e-10}, mean: -0.40204, std: 0.01918, params: {'alpha': 1e-08}, mean: -0.40199, std: 0.01919, params: {'alpha': 1e-06}, mean: -0.40165, std: 0.01915, params: {'alpha': 1e-05}, mean: -0.40067, std: 0.01889, params: {'alpha': 5e-05}, mean: -0.39957, std: 0.01857, params: {'alpha': 0.0001}, mean: -0.39354, std: 0.01774, params: {'alpha': 0.0005}, mean: -0.38985, std: 0.01766, params: {'alpha': 0.001}, mean: -0.37231, std: 0.01205, params: {'alpha': 0.01}, mean: -0.38565, std: 0.01529, params: {'alpha': 0.1}, mean: -1.01064, std: 0.07205, params: {'alpha': 1.0}]\n",
      "train mean squared error: 0.351477622081\n",
      "test mean squared error: 0.347345997444 \n",
      "\n",
      "train score: 0.648522377918\n",
      "test score: 0.665527793427 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The lasso regression model\n",
    "fitmodel = cv_optimize_lasso(X_train, Y_train, n_folds=4)\n",
    "\n",
    "print fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_\n",
    "\n",
    "alphawechoose = fitmodel.best_params_['alpha']\n",
    "clf_lasso = Lasso(alpha=alphawechoose).fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "errtr=mean_squared_error(clf_lasso.predict(X_train), Y_train)\n",
    "err=mean_squared_error(clf_lasso.predict(X_test), Y_test)\n",
    "\n",
    "print 'train mean squared error:', errtr\n",
    "print 'test mean squared error:', err, '\\n'\n",
    "\n",
    "training_accuracy = clf_lasso.score(X_train, Y_train)\n",
    "test_accuracy = clf_lasso.score(X_test, Y_test)\n",
    "\n",
    "print 'train score:', training_accuracy\n",
    "print 'test score:', test_accuracy, '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train score increases from 0.12 to **0.65** and the test score increase from 0.09 to **0.67**. Also huge improvement here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict 'prob_x'\n",
    "\n",
    "### 'prob_x' refers to \"how probable do you think it is that this person will say 'yes' for you\", which we would like to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build models for \"prob_x\", which means how much the pariticipant thinks his/her date will say \"yes\" after the date. Similarly, we will here Ridge and Lasso to build the models. \n",
    "\n",
    "First, we don't include these 12 rating features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does not consider the following features:\n",
    "    \n",
    "#### 'attr_x', 'sinc_x', 'intel_x', 'fun_x', 'amb_x', 'shar_x', \n",
    "\n",
    "#### 'attr_y', 'sinc_y', 'intel_y', 'fun_y', 'amb_y', 'shar_y',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5094, 172)\n",
      "(2184, 172)\n",
      "(5094, 1)\n",
      "(2184, 1)\n"
     ]
    }
   ],
   "source": [
    "#The third pair(ridge + lasso) of model: \n",
    "#Predict 'prob_x'\n",
    "#Does not consider the following features:\n",
    "#'attr_x', 'sinc_x', 'intel_x', 'fun_x', 'amb_x', 'shar_x', \n",
    "#'attr_y', 'sinc_y', 'intel_y', 'fun_y', 'amb_y', 'shar_y',\n",
    "\n",
    "X_train = dftrain[reg_features]\n",
    "X_test  = dftest[reg_features]\n",
    "Y_train = dftrain[reg_response_prob]\n",
    "Y_test  = dftest[reg_response_prob]\n",
    "\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print Y_train.shape\n",
    "print Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, solver='auto', tol=0.001) {'alpha': 1.0} -1.01440570686 [mean: -1.01989, std: 0.05993, params: {'alpha': 1e-10}, mean: -1.01989, std: 0.05992, params: {'alpha': 1e-08}, mean: -1.01989, std: 0.05992, params: {'alpha': 1e-06}, mean: -1.01989, std: 0.05992, params: {'alpha': 1e-05}, mean: -1.01989, std: 0.05992, params: {'alpha': 5e-05}, mean: -1.01989, std: 0.05992, params: {'alpha': 0.0001}, mean: -1.01988, std: 0.05992, params: {'alpha': 0.0005}, mean: -1.01988, std: 0.05992, params: {'alpha': 0.001}, mean: -1.01981, std: 0.05992, params: {'alpha': 0.01}, mean: -1.01918, std: 0.05988, params: {'alpha': 0.1}, mean: -1.01441, std: 0.06006, params: {'alpha': 1.0}]\n",
      "train mean squared error: 0.811496490692\n",
      "test mean squared error: 0.843180667199 \n",
      "\n",
      "train score: 0.188503509308\n",
      "test score: 0.161076004389 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The ridge regression model\n",
    "fitmodel = cv_optimize_ridge(X_train, Y_train, n_folds=4)\n",
    "\n",
    "print fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_\n",
    "\n",
    "alphawechoose = fitmodel.best_params_['alpha']\n",
    "clf_reg = Ridge(alpha=alphawechoose).fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "errtr=mean_squared_error(clf_reg.predict(X_train), Y_train)\n",
    "err=mean_squared_error(clf_reg.predict(X_test), Y_test)\n",
    "\n",
    "print 'train mean squared error:', errtr\n",
    "print 'test mean squared error:', err, '\\n'\n",
    "\n",
    "training_accuracy = clf_reg.score(X_train, Y_train)\n",
    "test_accuracy = clf_reg.score(X_test, Y_test)\n",
    "\n",
    "print 'train score:', training_accuracy\n",
    "print 'test score:', test_accuracy, '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train score is 0.19 and the test score is 0.16, which are quite low scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regressoin Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False) {'alpha': 0.1} -0.944599820235 [mean: -1.02027, std: 0.06008, params: {'alpha': 1e-10}, mean: -1.02027, std: 0.06008, params: {'alpha': 1e-08}, mean: -1.02026, std: 0.06011, params: {'alpha': 1e-06}, mean: -1.01990, std: 0.06025, params: {'alpha': 1e-05}, mean: -1.01784, std: 0.06071, params: {'alpha': 5e-05}, mean: -1.01547, std: 0.06131, params: {'alpha': 0.0001}, mean: -1.00266, std: 0.06509, params: {'alpha': 0.0005}, mean: -0.99275, std: 0.06954, params: {'alpha': 0.001}, mean: -0.95599, std: 0.08807, params: {'alpha': 0.01}, mean: -0.94460, std: 0.08593, params: {'alpha': 0.1}, mean: -1.00012, std: 0.09596, params: {'alpha': 1.0}]\n",
      "train mean squared error: 0.936762292283\n",
      "test mean squared error: 0.93789909787 \n",
      "\n",
      "train score: 0.0632377077174\n",
      "test score: 0.0668357455598 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The lasso regression model\n",
    "fitmodel = cv_optimize_lasso(X_train, Y_train, n_folds=4)\n",
    "\n",
    "print fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_\n",
    "\n",
    "alphawechoose = fitmodel.best_params_['alpha']\n",
    "clf_lasso = Lasso(alpha=alphawechoose).fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "errtr=mean_squared_error(clf_lasso.predict(X_train), Y_train)\n",
    "err=mean_squared_error(clf_lasso.predict(X_test), Y_test)\n",
    "\n",
    "print 'train mean squared error:', errtr\n",
    "print 'test mean squared error:', err, '\\n'\n",
    "\n",
    "training_accuracy = clf_lasso.score(X_train, Y_train)\n",
    "test_accuracy = clf_lasso.score(X_test, Y_test)\n",
    "\n",
    "print 'train score:', training_accuracy\n",
    "print 'test score:', test_accuracy, '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train score is 0.06 and the test score is 0.07, which are even lower than expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the following features:\n",
    "\n",
    "### 'attr_x', 'sinc_x', 'intel_x', 'fun_x', 'amb_x', 'shar_x', \n",
    "\n",
    "### 'attr_y', 'sinc_y', 'intel_y', 'fun_y', 'amb_y', 'shar_y',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5094, 184)\n",
      "(2184, 184)\n",
      "(5094, 1)\n",
      "(2184, 1)\n"
     ]
    }
   ],
   "source": [
    "#The fourth pair(ridge + lasso) of model: \n",
    "#Predict 'prob_x'\n",
    "#Consider the following features:\n",
    "#'attr_x', 'sinc_x', 'intel_x', 'fun_x', 'amb_x', 'shar_x', \n",
    "#'attr_y', 'sinc_y', 'intel_y', 'fun_y', 'amb_y', 'shar_y',\n",
    "\n",
    "X_train = dftrain[reg_features_pl]\n",
    "X_test  = dftest[reg_features_pl]\n",
    "Y_train = dftrain[reg_response_prob]\n",
    "Y_test  = dftest[reg_response_prob]\n",
    "\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print Y_train.shape\n",
    "print Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, solver='auto', tol=0.001) {'alpha': 1.0} -0.858185153733 [mean: -0.86494, std: 0.03529, params: {'alpha': 1e-10}, mean: -0.86494, std: 0.03529, params: {'alpha': 1e-08}, mean: -0.86494, std: 0.03529, params: {'alpha': 1e-06}, mean: -0.86494, std: 0.03529, params: {'alpha': 1e-05}, mean: -0.86494, std: 0.03529, params: {'alpha': 5e-05}, mean: -0.86494, std: 0.03529, params: {'alpha': 0.0001}, mean: -0.86493, std: 0.03529, params: {'alpha': 0.0005}, mean: -0.86493, std: 0.03529, params: {'alpha': 0.001}, mean: -0.86485, std: 0.03529, params: {'alpha': 0.01}, mean: -0.86409, std: 0.03532, params: {'alpha': 0.1}, mean: -0.85819, std: 0.03615, params: {'alpha': 1.0}]\n",
      "train mean squared error: 0.650485861202\n",
      "test mean squared error: 0.66522751682 \n",
      "\n",
      "train score: 0.349514138798\n",
      "test score: 0.338130784883 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The ridge regression model\n",
    "fitmodel = cv_optimize_ridge(X_train, Y_train, n_folds=4)\n",
    "\n",
    "print fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_\n",
    "\n",
    "alphawechoose = fitmodel.best_params_['alpha']\n",
    "clf_reg = Ridge(alpha=alphawechoose).fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "errtr=mean_squared_error(clf_reg.predict(X_train), Y_train)\n",
    "err=mean_squared_error(clf_reg.predict(X_test), Y_test)\n",
    "\n",
    "print 'train mean squared error:', errtr\n",
    "print 'test mean squared error:', err, '\\n'\n",
    "\n",
    "training_accuracy = clf_reg.score(X_train, Y_train)\n",
    "test_accuracy = clf_reg.score(X_test, Y_test)\n",
    "\n",
    "print 'train score:', training_accuracy\n",
    "print 'test score:', test_accuracy, '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train score increases from 0.19 to **0.35**, and the test score increases from 0.16 to **0.34**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False) {'alpha': 0.1} -0.769804680449 [mean: -0.86492, std: 0.03532, params: {'alpha': 1e-10}, mean: -0.86492, std: 0.03532, params: {'alpha': 1e-08}, mean: -0.86487, std: 0.03531, params: {'alpha': 1e-06}, mean: -0.86438, std: 0.03537, params: {'alpha': 1e-05}, mean: -0.86184, std: 0.03599, params: {'alpha': 5e-05}, mean: -0.85876, std: 0.03671, params: {'alpha': 0.0001}, mean: -0.84086, std: 0.04268, params: {'alpha': 0.0005}, mean: -0.82939, std: 0.05128, params: {'alpha': 0.001}, mean: -0.77913, std: 0.07397, params: {'alpha': 0.01}, mean: -0.76980, std: 0.07100, params: {'alpha': 0.1}, mean: -1.00012, std: 0.09596, params: {'alpha': 1.0}]\n",
      "train mean squared error: 0.759926643828\n",
      "test mean squared error: 0.743512808091 \n",
      "\n",
      "train score: 0.240073356172\n",
      "test score: 0.260240705205 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The lasso regression model\n",
    "fitmodel = cv_optimize_lasso(X_train, Y_train, n_folds=4)\n",
    "\n",
    "print fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_\n",
    "\n",
    "alphawechoose = fitmodel.best_params_['alpha']\n",
    "clf_lasso = Lasso(alpha=alphawechoose).fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "errtr=mean_squared_error(clf_lasso.predict(X_train), Y_train)\n",
    "err=mean_squared_error(clf_lasso.predict(X_test), Y_test)\n",
    "\n",
    "print 'train mean squared error:', errtr\n",
    "print 'test mean squared error:', err, '\\n'\n",
    "\n",
    "training_accuracy = clf_lasso.score(X_train, Y_train)\n",
    "test_accuracy = clf_lasso.score(X_test, Y_test)\n",
    "\n",
    "print 'train score:', training_accuracy\n",
    "print 'test score:', test_accuracy, '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train score increases from 0.06 to **0.24** and the test score increases from 0.07 to **0.26**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Ridge regression model does a better job predicting 'prob_x'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For predicting both 'like_x' and 'prob_x', models with the features: 'Attractive'(attr), 'Sincere'(sinc), 'Intelligent'(intel), 'Fun'(fun), 'Ambitious'(amb), 'Shared Interests/Hobbies'(shar) do a better job than models without these features.\n",
    "\n",
    "This indicates that how a person evaluates his or her partner is more related to the partner's personal attributes than external conditions such as career. what a person seeks in an ideal partner is \"a good feeling\" instead of \"a good match\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
